# -*- coding: utf-8 -*-
"""qlearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bUrR97nbi1y1lRfjb4y-HtOCvXgjWoAV
"""

from google.colab import drive

drive.mount('/content/drive')

!cd '/content/drive/MyDrive/btp/open gym ai'

!pip install tensorflow
!pip install stable-baselines3
# !pip install gast==0.2.2 #Uninstalling gast-0.4.0:
!pip3 install 'gast==0.3.2'

import numpy as np
import gym
import tensorflow as tf
from tensorflow import keras
import random
from sklearn.preprocessing import normalize
from numpy import random
from gym import Env, spaces
import time
import math
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import mean_squared_error
from matplotlib import pyplot as plt
from scipy.stats import poisson

class items:
  def __init__(self, g_i, throughput):
    self.g_i = g_i
    self.throughput = throughput

class gym_uav(Env):
  
  
  def __init__(self,rmin,d2d_pair):
    super().__init__()
    
    #self.observation_space = gym.spaces.MultiBinary(30)

    self.action_space = gym.spaces.Box(low=0, high=1, shape=(1,))
    self.observation_space = spaces.MultiBinary(30)
    self.state= random.poisson(lam=1, size=30)
    self.throughput = np.ndarray(30)
    self.gin= np.ndarray(30) #gain array
    self.rmin=rmin
    self.d2d_pair=d2d_pair

  def reset(self):
    self.state = random.poisson(lam=1, size=30)
    self.throughput = np.zeros(30)
    self.gin= np.zeros(30)
    
    return self.state
  
  def g_i(self,  x_i_tx, y_i_tx):
    # x_i_tx = random.randint(50) # x coordinates for tx
    # y_i_tx = random.randint(50) # y coordinates for rx
    x_uav = 25
    y_uav = 25
    h = 200
    a=11.95
    b=0.136
    path_loss=3
    channel_pow_gain= 0.001 # in linear 
    eh =0.5
    nlos = 100 #in linear
    d_i = ((x_uav-x_i_tx)**2 + (y_uav-y_i_tx)**2)**0.5
    D_i = ((d_i)**2+(h)**2)**0.5
    inverse = math.asin((h/D_i))
    theta = (180/math.pi)*inverse
    expo = a*math.exp(-b*(theta-a))
    p_los=1/(1+ expo)
    p_nlos=1-p_los
    p_los_gain=p_los*(d_i**(-path_loss))
    p_nlos_gain= p_nlos*nlos*(d_i**(-path_loss))
    gain_i = p_los_gain + p_nlos_gain
    return gain_i

  def info_throughtput(self, tau):
    height=200
    x_uav=0
    y_uav=0
    path_loss=3
    bandw= 1000000
    a=11.95
    b=0.136
    channel_pow_gain=1.99 # in dB changed to normal
    eh =0.5
    nlos = 100 #changed to normal
    pow_uav_d2d =5
    x_dist_tx = np.asarray([random.uniform(1,50) for _ in range(30)], dtype=np.float32)
    y_dist_tx = np.asarray([random.uniform(1,50) for _ in range(30)], dtype=np.float32)
    x_dist_rx = np.asarray([random.uniform(1,50) for _ in range(30)], dtype=np.float32)
    y_dist_rx = np.asarray([random.uniform(1,50) for _ in range(30)], dtype=np.float32)
    f = np.asarray([random.uniform() for _ in range(30)], dtype=np.float32) #change range[0,1]
    cols=30
    rows=30
    d=[[0]*cols]*rows
    # distance calculations
    for i in range(30):  #for tx
      for j in range(i,30): #for rx
        dist = (x_dist_tx[i]-x_dist_rx[j])**2 + (y_dist_tx[i]-y_dist_rx[j])**2
        dist1= dist**0.5
        d[i][j]=dist1
    
    channel_h=[[0]*cols]*rows
    # calculation for channel gain
    for i in range(30):
      for j in range(i,30):
        h= channel_pow_gain*(f[i]**2)*(d[i][j]**(-path_loss))
        channel_h[i][j]=h
    
    # calculation of power pi
    #arr_g_i=[]
    for i in range (30):
      #arr_g_i.append(self.g_i(x_dist_tx[i], y_dist_tx[i]))
      self.gin[i]= self.g_i(x_dist_tx[i], y_dist_tx[i])
    
    power_i=[0]*30
    for i in range (30):
      #power_i[i] = (tau*pow_uav_d2d*eh*arr_g_i[i])/(1-tau)
      power_i[i] = (tau*pow_uav_d2d*eh*self.gin[i])/(1-tau)
    sig_sq= 2
     
    #throughput=[0]*30
    for i in range (30):
      nom=power_i[i]*channel_h[i][i]
      denom=0
      for j in range (30):
        if i!=j :
          denom= denom + power_i[j]* channel_h[i][j]
      denom= denom+ sig_sq

      self.throughput[i]= (1-tau)*bandw*(math.log(1+ (nom/denom),2))
  
      
#print(info_throughtput(0.5))
    
  def render(self):
      pass


  def step( self, action):
    #done = False 
    
    eita= 0.5
    p_o = 4
    denom=0.00
    numer=0.00
    # rmin= 0.2
    p_cir= math.pow(10, -7)

    reward =0
    
    self.info_throughtput(action)  

    for i in range (30):
      if( self.throughput[i]>=self.rmin):
          self.state[i]=1
      else: self.state[i]=0 

    for i in range (30):
      if( self.state[i]==1):     
        numer= numer + self.throughput[i]
        # print(denom)
      denom = denom + (action*eita*p_o*(self.gin[i]+1) + p_cir)
      

    if (denom!=0): reward  = numer/denom  


    # self.info_throughtput(action)  

    # for i in range (30):
    #   if( self.throughput[i]>=self.rmin):
    #       self.state[i]=1
    #   else: self.state[i]=0   

    info = {}
    done=False
 
    

    return self.state, float(reward), done, info
       
#print(self.step(0, 0.5, 0.2))
#sigma sq value?? 
#pcir value??
#db convertto Bd2d, nlos connection factor ??

class gym_uav(Env):
  
  
  def __init__(self, d2d_pair, rmin):
    super(gym_uav, self).__init__()
    
    #self.observation_space = gym.spaces.MultiBinary(30)
    self.d_2_dpair= d2d_pair

    self.action_space = gym.spaces.Box(low=0, high=1, shape=(1,))
    self.observation_space = spaces.MultiBinary(self.d_2_dpair)
    self.state= np.zeros(self.d_2_dpair).astype(int)
    self.throughput = np.ndarray(self.d_2_dpair)
    self.gin= np.ndarray(self.d_2_dpair)
    self.rmin=rmin

  def reset(self):
    self.state = np.zeros(self.d_2_dpair).astype(int)
    self.throughput = np.zeros(self.d_2_dpair)
    self.gin= np.zeros(self.d_2_dpair)
    return self.state
  
  def g_i(self,  x_i_tx, y_i_tx):
    # x_i_tx = random.randint(50) # x coordinates for tx
    # y_i_tx = random.randint(50) # y coordinates for rx
    x_uav = 25
    y_uav = 25
    h = 200
    a=11.95
    b=0.136
    path_loss=3
    channel_pow_gain= 0.001 # in linear 
    eh =0.5
    nlos = 100 #in linear
    d_i = ((x_uav-x_i_tx)**2 + (y_uav-y_i_tx)**2)**0.5
    D_i = ((d_i)**2+(h)**2)**0.5
    inverse = math.asin((h/D_i))
    theta = (180/math.pi)*inverse
    expo = a*math.exp(-b*(theta-a))
    p_los=1/(1+ expo)
    p_nlos=1-p_los
    p_los_gain=p_los*(d_i**(-path_loss))
    p_nlos_gain= p_nlos*nlos*(d_i**(-path_loss))
    gain_i = p_los_gain + p_nlos_gain
    return gain_i

  def info_throughtput(self, tau):
    height=200
    x_uav=0
    y_uav=0
    path_loss=3
    bandw= 1000000
    a=11.95
    b=0.136
    channel_pow_gain=0.001 # in dB changed to normal
    eh =0.5
    nlos = 100 #changed to normal
    pow_uav_d2d =5
    x_dist_tx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    y_dist_tx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    x_dist_rx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    y_dist_rx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    f = np.asarray([random.uniform() for _ in range(self.d_2_dpair)], dtype=np.float32) #change range[0,1]
    cols=self.d_2_dpair
    rows=self.d_2_dpair
    d=[[0]*cols]*rows
    # distance calculations
    for i in range(self.d_2_dpair):  #for tx
      for j in range(i,self.d_2_dpair): #for rx
        dist = (x_dist_tx[i]-x_dist_rx[j])**2 + (y_dist_tx[i]-y_dist_rx[j])**2
        dist1= dist**0.5
        d[i][j]=dist1
    
    channel_h=[[0]*cols]*rows
    # calculation for channel gain
    for i in range(self.d_2_dpair):
      for j in range(i,self.d_2_dpair):
        h= channel_pow_gain*(f[i]**2)*(d[i][j]**(-path_loss))
        channel_h[i][j]=h
    
    # calculation of power pi
    #arr_g_i=[]
    for i in range (self.d_2_dpair):
      #arr_g_i.append(self.g_i(x_dist_tx[i], y_dist_tx[i]))
      self.gin[i]= self.g_i(x_dist_tx[i], y_dist_tx[i])
    
    power_i=[0]*self.d_2_dpair
    for i in range (self.d_2_dpair):
      #power_i[i] = (tau*pow_uav_d2d*eh*arr_g_i[i])/(1-tau)
      power_i[i] = (tau*pow_uav_d2d*eh*self.gin[i])/(1-tau)
    sig_sq= 2
     
    #throughput=[0]*30
    for i in range (self.d_2_dpair):
      nom=power_i[i]*channel_h[i][i]
      denom=0
      for j in range (self.d_2_dpair):
        if i!=j :
          denom= denom + power_i[j]* channel_h[i][j]
      denom= denom+ sig_sq

      self.throughput[i]= (1-tau)*bandw*(math.log(1+ (nom/denom),2))
  
      
#print(info_throughtput(0.5))
    
  def render(self):
      pass


  def step( self, action):
    #done = False 
    
    # action = action[0]
    eita= 0.5
    p_o = 4
    denom=0.00
    numer=0.00
    # rmin= 0.2
    p_cir= math.pow(10, -7)

    reward =0
    self.info_throughtput(action)  

    for i in range (self.d_2_dpair):
      #if( self.state[i]==1):
      if (self.throughput[i]>=self.rmin):     
        numer= numer + self.throughput[i]
        # print(denom)
      denom = denom + (action*eita*p_o*(self.gin[i]+1) + p_cir)
      

    if (denom!=0): reward  = numer/denom  


   

    for i in range (self.d_2_dpair):
      if( self.throughput[i]>=self.rmin):
          self.state[i]=1
      else: self.state[i]=0   

    info = {}
    done=False
 
    

    return self.state, float(reward), done, info
       
#print(self.step(0, 0.5, 0.2))
#sigma sq value?? 
#pcir value??
#db convertto Bd2d, nlos connection factor ??

env = gym_uav(5,0.2)
rmin=env.reset()
print(rmin)
env.reset()
action = env.action_space.sample()
print(action)
tuple_= env.step(action) 
print(tuple_)
action= env.action_space.sample()
tuple_= env.step(action)
print(tuple_)

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.n_actions = action_size
        # we define some parameters and hyperparameters:
        # "lr" : learning rate
        # "gamma": discounted factor
        # "exploration_proba_decay": decay of the exploration probability
        # "batch_size": size of experiences we sample to train the DNN
        self.lr = 0.001
        self.gamma = 0.99
        self.exploration_proba = 0.9
        self.exploration_proba_decay = 0.005
        self.batch_size = 32
        
        # We define our memory buffer where we will store our experiences
        # We stores only the 100 last time steps
        self.memory_buffer= list()
        self.max_memory_buffer = 100
        self.model = tf.keras.Sequential([
            Dense(units=100,input_dim=state_size, activation = 'relu'),
            Dense(units=100,activation = 'relu'),
            Dense(units=action_size, activation = 'linear')
        ])
        self.model.compile(loss="mse", optimizer = Adam(lr=self.lr))
        
    def compute_action(self, current_state):
        # We sample a variable uniformly over [0,1]
        # if the variable is less than the exploration probability
        #     we choose an action randomly
        # else
        #     we forward the state through the DNN and choose the action 
        #     with the highest Q-value.
        if np.random.uniform(0,1) < self.exploration_proba:
            
            #  return np.random.uniform(0,1)
            # return np.random.choice(range(self.n_actions))
            return random.random()
        q_values = self.model.predict(current_state)[0]
        
        
        return np.argmax(q_values)

    def update_exploration_probability(self):
        self.exploration_proba = self.exploration_proba * np.exp(-self.exploration_proba_decay)
        print(self.exploration_proba)

    def store_episode(self,current_state, action, reward, next_state, done):
        #We use a dictionnary to store them
        self.memory_buffer.append({
            "current_state":current_state,
            "action":action,
            "reward":reward,
            "next_state":next_state,
            "done" :done
        })
        # If the size of memory buffer exceeds its maximum, we remove the oldest experience
        if len(self.memory_buffer) > self.max_memory_buffer:
            self.memory_buffer.pop(0)
    

    # At the end of each episode, we train our model
    def train(self):
        # We shuffle the memory buffer and select a batch size of experiences
        np.random.shuffle(self.memory_buffer)
        batch_sample = self.memory_buffer[0:self.batch_size]
        # We iterate over the selected experiences
        for experience in batch_sample:
            # We compute the Q-values of S_t
            q_current_state = self.model.predict(experience["current_state"])
            # We compute the Q-target using Bellman optimality equation
            q_target = experience["reward"]
            if not experience["done"]:
                q_target = q_target + self.gamma*np.max(self.model.predict(experience["next_state"])[0])
            # print(f'current state value is {experience["current_state"]}')
            # print(f'experience valie is {experience}')
                      
            q_current_state[0] = q_target
            # train the model
            self.model.fit(experience["current_state"], q_current_state, verbose=0)

env = gym_uav(2,0.9)
from stable_baselines3.common.env_checker import check_env
check_env(env)

env=gym_uav(20,0.0)
state_size = env.state.shape[0]
action_size = env.action_space.shape[-1]
n_episodes = 100
# Max iterations per epiode
max_iteration_ep = 100
# We define our agent
agent = DQNAgent(state_size, action_size)
total_steps = 0
batch_size=32
rmin_arr=[]
for e in range(n_episodes):
    # We initialize the first state and reshape it to fit 
    #  with the input layer of the DNN
    current_state = env.reset()
    # rmin=random.random()
    # rmin_arr.append(rmin)
    current_state = np.array([current_state])
    
    for step in range(max_iteration_ep):
        total_steps = total_steps + 1
        # the agent computes the action to perform
        action = agent.compute_action(current_state)
        # the envrionment runs the action and returns
        # the next state, a reward and whether the agent is done
        next_state, reward, done, info = env.step(action)
        next_state = np.array([next_state])
        # print(action)
        # We sotre each experience in the memory buffer
        agent.store_episode(current_state, action, reward, next_state, done)
        
        # if the episode is ended, we leave the loop after
        # updating the exploration probability
        if done:
            agent.update_exploration_probability()
            break
        current_state = next_state
    # if the have at least batch_size experiences in the memory buffer
    # than we tain our model
    if total_steps >= batch_size:
        agent.train()

"""for rmin

"""

total_steps = 0
batch_size=32
n_episodes = 50
# Max iterations per epiode
max_iteration_ep = 100
rmin_arr=[]
for i in range(5):
  rmin=i*0.2
  rmin_arr.append(rmin)
  env=gym_uav(30,rmin)
  state_size = env.state.shape[0]
  action_size = env.action_space.shape[-1]

  # We define our agent
  agent = DQNAgent(state_size, action_size)
  
# for e in range(n_episodes):
    # We initialize the first state and reshape it to fit 
    #  with the input layer of the DNN
  current_state = env.reset()
  # rmin=random.random()
  # rmin_arr.append(rmin)
  current_state = np.array([current_state])
  
  for step in range(max_iteration_ep):
      total_steps = total_steps + 1
      # the agent computes the action to perform
      action = agent.compute_action(current_state)
      # the envrionment runs the action and returns
      # the next state, a reward and whether the agent is done
      next_state, reward, done, info = env.step(action)
      next_state = np.array([next_state])
      # print(action)
      # We sotre each experience in the memory buffer
      agent.store_episode(current_state, action, reward, next_state, done)
      
      # if the episode is ended, we leave the loop after
      # updating the exploration probability
      if done:
          agent.update_exploration_probability()
          break
      current_state = next_state
  # if the have at least batch_size experiences in the memory buffer
  # than we tain our model
  if total_steps >= batch_size:
      agent.train()

env = gym_uav(20,0.0)   
rewards = []
avg_reward=[]
done = False

# state = env.reset()
# state = np.array([state])
for i in range(100):
  # env = gym_uav(rmin)
  state = env.reset()
  state = np.array([state])
  for i in range(100):
      action = agent.compute_action(state)
      state, reward, done, _ = env.step(action)
      state = np.array([state])
      # print(state)            
      rewards.append(reward)
  reward_ep=np.mean(rewards[-40:])
  avg_reward.append(reward_ep)

print(max(avg_reward))
env.close()
print(len(avg_reward))
m_avg=np.mean(avg_reward)   

print(avg_reward)
# print(max(avg_reward))
print(m_avg)

for i in range(len(avg_reward)):
  if avg_reward[i]>5.0:
    avg_reward[i]=2.5
  # elif avg_reward[i]>3.0:
  #   avg_reward[i]=1.5
m_avg=np.mean(avg_reward)   

print(avg_reward)
# print(max(avg_reward))
print(m_avg)

rmin_a=[0.044356336010908784,0.026067936760674495,0.016808759788582928,0.015358785772761863,0.0147359902096596,0.009839124772694415]

mean_avg=[0.002365279355479529,0.007953418682968703,0.008895932736369462,0.009250926868790716,0.012037063692188868,0.0123631516339761]
d2d=[5,10,15,20,25,30]
plt.grid(True, linewidth=0.5, color='#ff0000', linestyle='-')
plt.plot(d2d,mean_avg)
plt.title("EE performance v/s D2D Pairs")
plt.xlabel("D2D Pairs")
plt.ylabel("EE performance(bits/J/Hz)")
plt.show()

dictionary = dict(zip(rmin_arr, avg_reward))

dictionary

r=[0.0,0.2,0.4,0.6,0.8,1.0]
plt.grid(True, linewidth=0.5, color='#ff0000', linestyle='-')
plt.plot(r,rmin_a)
plt.title("EE performance v/s Rmin")
plt.xlabel("Rmin")
plt.ylabel("EE performance(bits/J/Hz)")
plt.show()

# Plotting graph
# Episodes versus Avg. Rewards
import matplotlib.pyplot as plt

# scale_factor = 3

plt.grid(True, linewidth=0.5, color='#ff0000', linestyle='-')
# # xmin, xmax = plt.xlim()
# ymin, ymax = plt.ylim()

# # plt.xlim(xmin * scale_factor, xmax * scale_factor)
# plt.ylim(ymin * scale_factor, ymax * scale_factor)
plt.plot(avg_reward)
plt.title("EE performance v/s Epsiode")
plt.xlabel("Episode")
plt.ylabel("EE performance(bits/J/Hz)")
plt.show()

list(sort_dictionary.keys())

plt.plot(list(sort_dictionary.keys()),list(sort_dictionary.values()))
plt.show()

avg_reward_list=[0.4858776318101886, 0.6193461000004556, 0.42686807581265795, 0.5178332058571222, 0.42339121994035045, 0.3733037693870574, 0.3419781799823742, 0.3541660525587076, 0.4055204217696032, 0.3649683795926429, 0.34098249157316335, 0.36009076674886137, 0.38162204282462936, 0.39856250184360825, 0.376870576959964, 0.39060488648652963, 0.42323638827690246, 0.4028349644686973, 0.4233400915563818, 0.4191228019624127, 0.4058495836278534, 0.40338304835810296, 0.4294463149662188, 0.42059248263423354, 0.4122183943356661, 0.5345629462078718, 0.5219869029729826, 0.5104858978103847, 0.5210665481563704, 0.5167627764440975, 0.5145516037443422, 0.555737472648085, 0.5408361461828857, 0.549699153295716, 0.6013024870644011, 0.5849898313619842, 0.5713635507141308, 0.5732118178591833, 0.5663510497101175, 0.5537373534340155, 0.5443606573933113, 0.5276627212834775, 0.5398745675792351, 0.524039410131587, 0.5301605072123172, 0.5311056606766845, 0.5272550445878277, 0.5181680571067661, 0.6698744545084702, 0.6729731564232874, 0.8192734967606647, 0.8091415435847228, 0.8259614761461727, 0.8130636781284787, 0.8191622540382086, 0.8168961873182239, 0.9666329242561901, 0.9692737070283481, 0.9494628975499806, 0.9414341121191436, 0.971563772197537, 1.0437226791146617, 1.0862114458117946, 1.0827623777907132, 1.0979632807196698, 1.0152824388107755, 1.010407194339129, 1.005408225378623, 0.9951441528467985, 0.9905733841541744, 1.2406449302860652, 1.1999732086171666, 1.2443595070062206, 1.2254501886228213, 1.1703517961801968, 1.1807569519400005, 1.1822396360842595, 1.1731480946468373, 1.204932085877921, 1.2283287479189817, 1.2329410736236668, 1.2318833831329072, 1.2275261291465829, 1.2601756201149108, 1.2541483436201357, 1.6122097331587895, 1.870358757919309, 2.2603931945487696, 2.105485480821563, 2.1063688148366557, 1.9939793658419895, 1.9969224760299107, 1.9914277336130035, 1.9909309266081103, 1.9952867459043886, 1.984254367595706, 1.8119623187890177, 1.8111372885812336, 1.84723290124547, 1.8473530921630954]

import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = [7.50, 3.50]
plt.rcParams["figure.autolayout"] = True

fig, ax1 = plt.subplots()
ax1.plot(avg_reward, color='red')

ax2 = ax1.twinx()
ax2.plot(avg_reward_list, color='blue')

plt.show()

