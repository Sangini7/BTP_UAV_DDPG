# -*- coding: utf-8 -*-
"""ddpg_btp1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IJabiS4BTlRFOJkYx1_0uP2kZ1H4QkD3
"""

from google.colab import drive
drive.mount('/content/drive')

cd '/content/drive/MyDrive/btp/open gym ai'

"""## Installation"""

pip install gast==0.2.2 #Uninstalling gast-0.4.0:

!pip install stable-baselines3

pip install tensorflow # %tensorflow_version 1.x

import numpy as np 
import cv2 
import matplotlib.pyplot as plt
from tensorflow.keras import layers
import PIL.Image as Image
import gym
import random
from sklearn.preprocessing import normalize
from numpy import random
from gym import Env, spaces
import time
import math
import tensorflow as tf

font = cv2.FONT_HERSHEY_COMPLEX_SMALL

class gym_uav(Env):
  
  
  def __init__(self, d2d_pair=30, rmin=0.2):
    super(gym_uav, self).__init__()
    
    #self.observation_space = gym.spaces.MultiBinary(30)
    self.d_2_dpair= d2d_pair

    self.action_space = gym.spaces.Box(low=0, high=1, shape=(1,))
    self.observation_space = spaces.MultiBinary(self.d_2_dpair)
    self.state= np.zeros(self.d_2_dpair).astype(int)
    self.throughput = np.ndarray(self.d_2_dpair)
    self.gin= np.ndarray(self.d_2_dpair)
    self.rmin=rmin

  def reset(self):
    self.state = np.zeros(self.d_2_dpair).astype(int)
    self.throughput = np.zeros(self.d_2_dpair)
    self.gin= np.zeros(self.d_2_dpair)
    return self.state
  
  def g_i(self,  x_i_tx, y_i_tx):
    # x_i_tx = random.randint(50) # x coordinates for tx
    # y_i_tx = random.randint(50) # y coordinates for rx
    x_uav = 25
    y_uav = 25
    h = 200
    a=11.95
    b=0.136
    path_loss=3
    channel_pow_gain= 0.001 # in linear 
    eh =0.5
    nlos = 100 #in linear
    d_i = ((x_uav-x_i_tx)**2 + (y_uav-y_i_tx)**2)**0.5
    D_i = ((d_i)**2+(h)**2)**0.5
    inverse = math.asin((h/D_i))
    theta = (180/math.pi)*inverse
    expo = a*math.exp(-b*(theta-a))
    p_los=1/(1+ expo)
    p_nlos=1-p_los
    p_los_gain=p_los*(d_i**(-path_loss))
    p_nlos_gain= p_nlos*nlos*(d_i**(-path_loss))
    gain_i = p_los_gain + p_nlos_gain
    return gain_i

  def info_throughtput(self, tau):
    height=200
    x_uav=0
    y_uav=0
    path_loss=3
    bandw= 1000000
    a=11.95
    b=0.136
    channel_pow_gain=0.001 # in dB changed to normal
    eh =0.5
    nlos = 100 #changed to normal
    pow_uav_d2d =5
    x_dist_tx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    y_dist_tx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    x_dist_rx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    y_dist_rx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    f = np.asarray([random.uniform() for _ in range(self.d_2_dpair)], dtype=np.float32) #change range[0,1]
    cols=self.d_2_dpair
    rows=self.d_2_dpair
    d=[[0]*cols]*rows
    # distance calculations
    for i in range(self.d_2_dpair):  #for tx
      for j in range(i,self.d_2_dpair): #for rx
        dist = (x_dist_tx[i]-x_dist_rx[j])**2 + (y_dist_tx[i]-y_dist_rx[j])**2
        dist1= dist**0.5
        d[i][j]=dist1
    
    channel_h=[[0]*cols]*rows
    # calculation for channel gain
    for i in range(self.d_2_dpair):
      for j in range(i,self.d_2_dpair):
        h= channel_pow_gain*(f[i]**2)*(d[i][j]**(-path_loss))
        channel_h[i][j]=h
    
    # calculation of power pi
    #arr_g_i=[]
    for i in range (self.d_2_dpair):
      #arr_g_i.append(self.g_i(x_dist_tx[i], y_dist_tx[i]))
      self.gin[i]= self.g_i(x_dist_tx[i], y_dist_tx[i])
    
    power_i=[0]*self.d_2_dpair
    for i in range (self.d_2_dpair):
      #power_i[i] = (tau*pow_uav_d2d*eh*arr_g_i[i])/(1-tau)
      power_i[i] = (tau*pow_uav_d2d*eh*self.gin[i])/(1-tau)
    sig_sq= 2
     
    #throughput=[0]*30
    for i in range (self.d_2_dpair):
      nom=power_i[i]*channel_h[i][i]
      denom=0
      for j in range (self.d_2_dpair):
        if i!=j :
          denom= denom + power_i[j]* channel_h[i][j]
      denom= denom+ sig_sq

      self.throughput[i]= (1-tau)*bandw*(math.log(1+ (nom/denom),2))
  
      
    
  def render(self):
      pass


  def step( self, action):
    #done = False 
    assert self.action_space.contains(action) #Invalid Action [1,1,1,1,1,1,1,0,0,0]
    action = action[0]
    eita= 0.5
    p_o = 4
    denom=0.00
    numer=0.00
    # rmin= 0.2
    p_cir= math.pow(10, -7)

    reward =0
    self.info_throughtput(action)  

    for i in range (self.d_2_dpair):
      #if( self.state[i]==1):
      if (self.throughput[i]>=self.rmin):     
        numer= numer + self.throughput[i]
        # print(denom)
      denom = denom + (action*eita*p_o*(self.gin[i]+1) + p_cir)
      

    if (denom!=0): reward  = numer/denom  


   

    for i in range (self.d_2_dpair):
      if( self.throughput[i]>=self.rmin):
          self.state[i]=1
      else: self.state[i]=0   

    info = {}
    done=False
 
    

    return self.state, float(reward), done, info

env = gym_uav()

env.reset()
action = env.action_space.sample()
print(action)

tuple_= env.step(action) 
print(tuple_)
action= env.action_space.sample()
tuple_= env.step(action)
print(tuple_)

"""## Checking gym compatibility """

num_states = env.observation_space.shape[0]
print("Size of State Space ->  {}".format(num_states))
num_actions = env.action_space.shape[0]
print("Size of Action Space ->  {}".format(num_actions))
upper_bound = env.action_space.high[0]
lower_bound = env.action_space.low[0]

print("Max Value of Action ->  {}".format(upper_bound))
print("Min Value of Action ->  {}".format(lower_bound))

no_of_d2d= 30
rmin=0.4
env = gym_uav(no_of_d2d, rmin)

from stable_baselines3.common.env_checker import check_env
check_env(env)

from stable_baselines3.ddpg.policies import MlpPolicy
from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec
from stable_baselines3 import DDPG
import matplotlib.pyplot as plt
#eeperf_mean_list = []
#for i in range (6,6):
#env = gym_uav()
n_actions = env.action_space.shape[-1]
# param_noise = None
action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1*np.ones(n_actions))

model = DDPG("MlpPolicy", env, verbose=1,action_noise=action_noise,actor_lr=0.0001, critic_lr=0.001,gamma= 0.99, batch_size = 32, tau=0.001, buffer_size=32, memory_policy=None)
model.learn(total_timesteps=500)
obs = env.reset() 
ep_reward_list = []
# To store average reward history of last few episodes
avg_reward_list = []
total_episodes = 100
# Takes about 4 min to train
for ep in range(total_episodes):

    prev_state = env.reset()
    episodic_reward = 0

    for i in range (100):
        # Uncomment this to see the Actor in action
        # But not in a python notebook.
        # env.render()

        #tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)

        #action = policy(tf_prev_state)
        action, _states = model.predict(obs)
        #action= [0.5]
        # Recieve state and reward from environment.
        state, reward, done, info = env.step(action)

        #buffer.record((prev_state, action, reward, state))
        episodic_reward += reward

        #buffer.learn()
        #update_target(target_actor.variables, actor_model.variables, tau)
        #update_target(target_critic.variables, critic_model.variables, tau)

        # End this episode when `done` is True
        if done:
            break

        prev_state = state

    ep_reward_list.append(episodic_reward)

    # Mean of last 40 episodes
    avg_reward = np.mean(ep_reward_list[-40:])
    print("Episode * {} * Avg Reward is ==> {}".format(ep, avg_reward))
    avg_reward_list.append(avg_reward)
#eeperf_mean_list.append(np.mean(avg_reward_list))
# Plotting graph
# Episodes versus Avg. Rewards
print("At the rmin given as = {} getting EE performance as {} ".format(rmin, np.mean(avg_reward_list)) )
plt.plot(avg_reward_list)
plt.xlabel("Episode")
plt.ylabel("Avg. Epsiodic Reward")
plt.show()
#plt.plot(eeperf_mean_list)
#plt.xlabel("Episode")
#plt.ylabel("Avg. Epsiodic Reward")
#plt.show()

print(avg_reward_list)

avg_reward=[1.3727377414933097,0.9671554941409246,0.705513997113906,0.5416038759389082, 0.433283100751126,0.600643167385832, 0.6123294235181582,0.568962497116584, 1.484935983473502, 1.339288705347912, 1.2219467876343328,1.1290746030356, 1.0548651227804944,1.053226337783111,0.9966324869605427,1.0286963672724907, 0.9735116702188144,0.9256440128510673,0.8908916026568937, 0.855494693079722,0.8758647587295433, 0.9638589540030736, 0.9419887697327624, 1.0253476983387342,0.9873696284265933,0.9513004023182609,0.9229512053113618, 0.9029765572294066,1.4625760821231157,1.4174858030154318,1.3793398470674125,1.3754924147904228,1.338719157623377,1.3047024676571455,1.2761940377456278,1.273034857867025,1.311616459550475,1.285664935252493,1.32425180068535,1.2946424252077793,1.2607040124940565,1.2490447064187606,1.2535277241193898,1.252930181177837,1.2581115938080116,1.223314973465734,1.2469913859878203,1.3174018744599576,1.2448085660129276,1.256069804066281,1.2598982859757821,1.2582338321171318,1.2554343312829321,1.2464566383857805,1.2477013416560336,1.2147032453064,1.2445744949354791,1.55307116121407,1.5503067854596981,1.6087150174647362,2.2389109728107486,2.2116358870576738,2.215463148229054,2.14718899852623,2.1484179816834215,2.1627690658572,2.1790595147534773,2.1899924684154235,1.7757189624447947,1.794720619728,1.791265987379943,1.765210374656149,1.7630472466265084,1.758842865743667,1.7924604513202471,1.7651769678070253,1.705679009640289,1.6990673877249587,1.6339141006574458,1.6605350628823374,1.8790401380762494,1.8783243852950036,1.8875792063415062,1.888020016425931,2.3132070059798187,2.3706616579807203,2.3488963728585555,2.3428458254633515,2.400339367691148,2.3992248603629234,2.411036455125511,2.4556829483633265,2.45691330813164,2.4465632307883105,2.4562672889521897,2.4550705275846596,2.4243919014006736,2.114028186144039,2.146435959842031,2.087763004380077]

# avg_reward=[0.4858776318101886, 0.6193461000004556, 0.42686807581265795, 0.5178332058571222, 0.42339121994035045, 0.3733037693870574, 0.3419781799823742, 0.3541660525587076, 0.4055204217696032, 0.3649683795926429, 0.34098249157316335, 0.36009076674886137, 0.38162204282462936, 0.39856250184360825, 0.376870576959964, 0.39060488648652963, 0.42323638827690246, 0.4028349644686973, 0.4233400915563818, 0.4191228019624127, 0.4058495836278534, 0.40338304835810296, 0.4294463149662188, 0.42059248263423354, 0.4122183943356661, 0.5345629462078718, 0.5219869029729826, 0.5104858978103847, 0.5210665481563704, 0.5167627764440975, 0.5145516037443422, 0.555737472648085, 0.5408361461828857, 0.549699153295716, 0.6013024870644011, 0.5849898313619842, 0.5713635507141308, 0.5732118178591833, 0.5663510497101175, 0.5537373534340155, 0.5443606573933113, 0.5276627212834775, 0.5398745675792351, 0.524039410131587, 0.5301605072123172, 0.5311056606766845, 0.5272550445878277, 0.5181680571067661, 0.6698744545084702, 0.6729731564232874, 0.8192734967606647, 0.8091415435847228, 0.8259614761461727, 0.8130636781284787, 0.8191622540382086, 0.8168961873182239, 0.9666329242561901, 0.9692737070283481, 0.9494628975499806, 0.9414341121191436, 0.971563772197537, 1.0437226791146617, 1.0862114458117946, 1.0827623777907132, 1.0979632807196698, 1.0152824388107755, 1.010407194339129, 1.005408225378623, 0.9951441528467985, 0.9905733841541744, 1.2406449302860652, 1.1999732086171666, 1.2443595070062206, 1.2254501886228213, 1.1703517961801968, 1.1807569519400005, 1.1822396360842595, 1.1731480946468373, 1.204932085877921, 1.2283287479189817, 1.2329410736236668, 1.2318833831329072, 1.2275261291465829, 1.2601756201149108, 1.2541483436201357, 1.6122097331587895, 1.870358757919309, 2.2603931945487696, 2.105485480821563, 2.1063688148366557, 1.9939793658419895, 1.9969224760299107, 1.9914277336130035, 1.9909309266081103, 1.9952867459043886, 1.984254367595706, 1.8119623187890177, 1.8111372885812336, 1.84723290124547, 1.8473530921630954]
plt.grid(True, linewidth=0.5, color='#ff0000', linestyle='-')
plt.title("EE performance v/s Episodes")
plt.plot(avg_reward)
plt.xlabel("Episodes")
plt.ylabel("EE performance(bits/J/Hz)")
plt.show()

x= [5,10,15,20,25,30]
y= [1.129, 1.349, 1.34, 1.75, 1.8, 2.06]
plt.grid(True, linewidth=0.5, color='#ff0000', linestyle='-')
plt.title("EE performance v/s D2D Pairs")
plt.plot(x,y)
plt.xlabel("No. of D2D pairs")
plt.ylabel("EE performance(bits/J/Hz)")
plt.show()

x= [0.2,0.4,0.6,0.8,1.0]
y= [1.6090, 1.519, 1.4997, 1.052, 0.700]
plt.grid(True, linewidth=0.5, color='#ff0000', linestyle='-')
plt.title("EE performance v/s Rmin")
plt.plot(x,y)
plt.xlabel("Rmin")
plt.ylabel("EE performance(bits/J/Hz)")
plt.show()

