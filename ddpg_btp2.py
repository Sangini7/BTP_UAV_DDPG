# -*- coding: utf-8 -*-
"""ddpg_btp2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17GyHIn168ep2Q_VGcFDntunJi5Y_GWrX
"""

from google.colab import drive
drive.mount('/content/drive')

cd '/content/drive/MyDrive/btp/open gym ai'

"""## Installation"""

pip install gast==0.2.2 #Uninstalling gast-0.4.0:

!pip install stable-baselines3

!pip install tensorflow # %tensorflow_version 1.x

# !pip install opencv-python 
# !pip install pillow

import numpy as np 
import cv2 
import matplotlib.pyplot as plt
from tensorflow.keras import layers
import PIL.Image as Image
import gym
import random
from sklearn.preprocessing import normalize
from numpy import random
from gym import Env, spaces
import time
import math
import tensorflow as tf


font = cv2.FONT_HERSHEY_COMPLEX_SMALL

class gym_uav(Env):
  
  
  def __init__(self, d2d_pair, rmin=0.2):
    super(gym_uav, self).__init__()
    
    #self.observation_space = gym.spaces.MultiBinary(30)
    self.epoch=0
    self.d_2_dpair= d2d_pair

    self.action_space = gym.spaces.Box(low=0, high=1, shape=(self.d_2_dpair,))
    self.observation_space = spaces.MultiBinary(self.d_2_dpair)
    self.state= np.zeros(self.d_2_dpair).astype(int)
    self.throughput = np.ndarray(self.d_2_dpair)
    self.gin= np.ndarray(self.d_2_dpair)
    self.rmin=rmin
    self.x_dist_tx=[]
    self.y_dist_tx=[]
    self.x_dist_rx=[]
    self.y_dist_rx=[]
    # self.x_dist_tx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    # self.y_dist_tx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    # self.x_dist_rx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    # self.y_dist_rx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    while len(self.x_dist_tx)!=30:
      x_distx=random.poisson(lam=1,size=None)
      if x_distx!=0:
        self.x_dist_tx.append(x_distx) 

    while len(self.y_dist_tx)!=30:
      y_distx=random.poisson(lam=1,size=None)
      if y_distx!=0:
        self.y_dist_tx.append(y_distx)

    while len(self.x_dist_rx)!=30:
      x_disrx=random.poisson(lam=1,size=None)
      if x_disrx!=0:
        self.x_dist_rx.append(x_disrx)
    
    while len(self.y_dist_rx)!=30:
      y_disrx=random.poisson(lam=1,size=None)
      if y_disrx!=0:
        self.y_dist_rx.append(y_disrx)

    
 

  def reset(self):
    # self.state = np.zeros(self.d_2_dpair).astype(int)
    # self.throughput = np.zeros(self.d_2_dpair)
    # self.gin= np.zeros(self.d_2_dpair)
    # self.x_dist_tx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    # self.y_dist_tx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    # self.x_dist_rx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    # self.y_dist_rx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    self.x_dist_tx=[]
    self.y_dist_tx=[]
    self.x_dist_rx=[]
    self.y_dist_rx=[]
    # self.x_dist_tx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    # self.y_dist_tx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    # self.x_dist_rx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    # self.y_dist_rx = np.asarray([random.uniform(1,50) for _ in range(self.d_2_dpair)], dtype=np.float32)
    while len(self.x_dist_tx)!=30:
      x_distx=random.poisson(lam=1,size=None)
      if x_distx!=0:
        self.x_dist_tx.append(x_distx) 

    while len(self.y_dist_tx)!=30:
      y_distx=random.poisson(lam=1,size=None)
      if y_distx!=0:
        self.y_dist_tx.append(y_distx)

    while len(self.x_dist_rx)!=30:
      x_disrx=random.poisson(lam=1,size=None)
      if x_disrx!=0:
        self.x_dist_rx.append(x_disrx)
    
    while len(self.y_dist_rx)!=30:
      y_disrx=random.poisson(lam=1,size=None)
      if y_disrx!=0:
        self.y_dist_rx.append(y_disrx)

    return self.state
  
  def g_i(self,  x_i_tx, y_i_tx):
    # x_i_tx = random.randint(50) # x coordinates for tx
    # y_i_tx = random.randint(50) # y coordinates for rx
    x_uav = 25
    y_uav = 25
    h = 200
    a=11.95
    b=0.136
    path_loss=3
    channel_pow_gain= 0.001 # in linear 
    eh =0.5
    nlos = 100 #in linear
    d_i = ((x_uav-x_i_tx)**2 + (y_uav-y_i_tx)**2)**0.5
    D_i = ((d_i)**2+(h)**2)**0.5
    inverse = math.asin((h/D_i))
    theta = (180/math.pi)*inverse
    expo = a*math.exp(-b*(theta-a))
    p_los=1/(1+ expo)
    p_nlos=1-p_los
    p_los_gain=p_los*(d_i**(-path_loss))
    p_nlos_gain= p_nlos*nlos*(d_i**(-path_loss))
    gain_i = p_los_gain + p_nlos_gain
    return gain_i

  def info_throughtput(self, tau):
   
    height=200
    x_uav=0
    y_uav=0
    path_loss=3
    bandw= 1000000
    a=11.95
    b=0.136
    channel_pow_gain=0.001 # in dB changed to normal
    eh =0.5
    nlos = 100 #changed to normal
    pow_uav_d2d =5
   
    
    
    
    for i in range(self.d_2_dpair):  #for tx
      r = 1 * random.uniform(0.0, 1.0)**0.5
      theta = random.uniform(0.0, 1.0) * 2 * math.pi
      self.x_dist_tx[i] = self.x_dist_tx[i] + r * math.cos(theta)
      self.y_dist_tx[i] = self.x_dist_tx[i] + r * math.sin(theta)
      if(self.x_dist_tx[i]<0): self.x_dist_tx[i]=0
      if(self.x_dist_tx[i]>50): self.x_dist_tx[i]=50
      if(self.y_dist_tx[i]<0): self.y_dist_tx[i]=0
      if(self.y_dist_tx[i]>50): self.y_dist_tx[i]=50
      
      self.x_dist_rx[i] = self.x_dist_rx[i] + r * math.cos(theta)
      self.y_dist_rx[i] = self.y_dist_rx[i] + r * math.sin(theta)
      if(self.x_dist_rx[i]<0): self.x_dist_rx[i]=0
      if(self.x_dist_rx[i]>50): self.x_dist_rx[i]=50
      if(self.y_dist_rx[i]<0): self.y_dist_rx[i]=0
      if(self.y_dist_rx[i]>50): self.y_dist_rx[i]=50


 
   

    f = np.asarray([random.uniform() for _ in range(self.d_2_dpair)], dtype=np.float32) #change range[0,1]
    cols=self.d_2_dpair
    rows=self.d_2_dpair
    d=[[0]*cols]*rows
    # distance calculations
    for i in range(self.d_2_dpair):  #for tx
      for j in range(i,self.d_2_dpair): #for rx
        dist = (self.x_dist_tx[i]-self.x_dist_rx[j])**2 + (self.y_dist_tx[i]-self.y_dist_rx[j])**2
        dist1= dist**0.5
        d[i][j]=dist1
    for i in range(len(d)):
      for j in range(len(d)):
        if d[i][j]==0:
          d[i][j]= 1
    #print(f'this is dist{d}')
    channel_h=[[0]*cols]*rows
    # calculation for channel gain
    for i in range(self.d_2_dpair):
      for j in range(i,self.d_2_dpair):
        h= channel_pow_gain*(f[i]**2)*(d[i][j]**(-path_loss))
        channel_h[i][j]=h
    
    # calculation of power pi
    #arr_g_i=[]
    for i in range (self.d_2_dpair):
      #arr_g_i.append(self.g_i(x_dist_tx[i], y_dist_tx[i]))
      self.gin[i]= self.g_i(self.x_dist_tx[i],self.y_dist_tx[i])
    
    power_i=[0]*self.d_2_dpair
    for i in range (self.d_2_dpair):
      #power_i[i] = (tau*pow_uav_d2d*eh*arr_g_i[i])/(1-tau)
      power_i[i] = (tau[i]*pow_uav_d2d*self.gin[i])/(1-tau[i])
    sig_sq= 2
    # print(f'this is tau{tau}') 
    #print(len(self.gin))
    nom = [0]*self.d_2_dpair
    
    #throughput=[0]*30
    for i in range (self.d_2_dpair):
      nom[i]=power_i[i]*channel_h[i][i]
      denom=0
      for j in range (self.d_2_dpair):
        if i!=j :
          denom= denom + power_i[j]* channel_h[i][j]
      denom= denom+ sig_sq
   
      self.throughput[i]= (1-tau[i])*bandw*(math.log(1+ (nom[i]/denom),2))
      # print(f'this is nom {nom[i]}')
    # print(f'this is throughput{self.throughput*1000000}')
    return nom
      
    
  
    
#print(info_throughtput(0.5))
    
  def render(self):
      pass


  def step( self, action):
    #done = False 
    # assert self.action_space.contains(action) #Invalid Action [1,1,1,1,1,1,1,0,0,0]
    # action = action[0]
    #self.epoch= self.epoch+1
    eita= 1
    
    p_o = 5*10**(-3)
    denom=0.00
    numer=1.00
    # rmin= 0.2
    p_cir= math.pow(10, -7)

    reward =0
    power_rec= self.info_throughtput(action)  
    
    for i in range (self.d_2_dpair):
      #if( self.state[i]==1):
      # print(f'through{self.throughput[i]*100000000}')
      if (self.throughput[i]*100000000>=self.rmin):     
        numer= numer + self.throughput[i]
        # print(denom)
      p_rec= power_rec[i]*10**12
      if p_rec<0.282:
        p_har=0
      elif p_rec>=0.282 and p_rec<0.501:
        p_har=0.857*p_rec-2.297*10**(-4)
      elif p_rec>=0.501 and p_rec<1.0:
        p_har=0.786*p_rec-1.941*10**(-4)
      elif p_rec>=1.0 and p_rec<3.548:
        p_har=0.485*p_rec+1.068*10**(-4)
      elif p_rec>=3.548 and p_rec<25.119:
        p_har=0.733*p_rec-7.724*10**(-4)
      elif p_rec>=25.119 and p_rec<100:
        p_har=0.465*p_rec+59.48*10**(-4)
      else:
        p_har=0.00001
      # print(f'this is prec {p_rec}')
      # print(f' pharvested {p_har}')
      denom = denom + p_har+ action[i]*p_o
      # print(f' demon {self.throughput[i]}')

      if (denom!=0):
        reward  = (numer/(denom+ p_cir))*10**5
    # print(f'reward {reward}')
    # print(f'denom {denom}')
    # print(f'num {numer}')

   

    for i in range (self.d_2_dpair):
      if( self.throughput[i]*100000000>=self.rmin):
          self.state[i]=1
      else: self.state[i]=0   

    info = {}
    done= False
    for i in range (self.d_2_dpair):
      if ( self.state[i]==0):
         break
      else:
        done= True    
 
    

    return self.state, float(reward), done, info

env = gym_uav(5)

env.reset()
action = env.action_space.sample()
print(action)
for i in range(100):
  tuple_= env.step(action) 
  action = env.action_space.sample()

"""## Checking gym compatibility """

no_of_d2d= 5
rmin=0.4
env = gym_uav(no_of_d2d, rmin)
# from stable_baselines.common.env_checker import check_env

# stable_baselines3.common.env_checker.check_env(env, warn=True, skip_render_check=True)
from stable_baselines3.common.env_checker import check_env
check_env(env)

from stable_baselines3.ddpg.policies import MlpPolicy
from stable_baselines3 import DDPG
from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise
import matplotlib.pyplot as plt

from stable_baselines3.ddpg.policies import MlpPolicy
from stable_baselines3 import DDPG
from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise
import matplotlib.pyplot as plt
#eeperf_mean_list = []
#for i in range (6,6):
#env = gym_uav()
n_actions = env.action_space.shape[-1]
# param_noise = None
action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1*np.ones(n_actions))

# model = DDPG("MlpPolicy", env, verbose=1,action_noise=action_noise,action_lr=0.001, critic_lr=0.001,gamma= 0.99, batch_size = 32, tau=0.001, buffer_size=32, memory_policy=None)
model = DDPG("MlpPolicy", env, action_noise=action_noise, verbose=1)
model.learn(total_timesteps=100)
print('done')
obs = env.reset() 
ep_reward_list = []
# To store average reward history of last few episodes
avg_reward_list = []
total_episodes = 100

# Takes about 4 min to train
for ep in range(total_episodes):

  prev_state = env.reset()
  episodic_reward = 0
  # for i in range(2):
  #   print('ip')
  
  for i in range (100):
  #     # Uncomment this to see the Actor in action
  #     # But not in a python notebook.
  #     # env.render()
      print('balle')
  #     #tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)

  #     #action = policy(tf_prev_state)
      action, _states = model.predict(prev_state)
      
      
  #     #action= [0.5]
  #     # Recieve state and reward from environment.
      state, reward, done, info = env.step(action)
      print(f'these are states {state}')

  #     #buffer.record((prev_state, action, reward, state))
      episodic_reward += reward

  #     #buffer.learn()
  #     #update_target(target_actor.variables, actor_model.variables, tau)
  #     #update_target(target_critic.variables, critic_model.variables, tau)

  #     # End this episode when `done` is True
      if done:
          break

      prev_state = state

  ep_reward_list.append(episodic_reward/500)

    # # Mean of last 40 episodes
  avg_reward = np.mean(ep_reward_list)
  print("Episode * {} * Avg Reward is ==> {}".format(ep, avg_reward))
  avg_reward_list.append(avg_reward)

print("At the rmin given as = {} getting EE performance as {} ".format(rmin, np.mean(avg_reward_list)) )
plt.grid(True, linewidth=0.5, color='#ff0000', linestyle='-')
plt.title("EE performance v/s Episodes")
plt.plot(avg_reward_list)
plt.xlabel("Episode")
plt.ylabel("Avg. Epsiodic Reward")
plt.show()
#plt.plot(eeperf_mean_list)
#plt.xlabel("Episode")
#plt.ylabel("Avg. Epsiodic Reward")
#plt.show()

#eeperf_mean_list = []
#for i in range (6,6):
#env = gym_uav()



action_list1 =  []
action_list2 =  []
action_list3 =  []
action_list4 =  []
action_list5 =  []

total_episodes = 200

# Takes about 4 min to train
for ep in range(total_episodes):
  n_actions = env.action_space.shape[-1]
  # param_noise = None
  action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1*np.ones(n_actions))

  # model = DDPG("MlpPolicy", env, verbose=1,action_noise=action_noise,action_lr=0.001, critic_lr=0.001,gamma= 0.99, batch_size = 32, tau=0.001, buffer_size=32, memory_policy=None)
  model = DDPG("MlpPolicy", env, action_noise=action_noise, verbose=1)
  obs = env.reset() 
  
  model.learn(total_timesteps=5)

  prev_state = [0,0,1,0,1]

  action, _states = model.predict(prev_state)
  action_list1.append(action[0])
  action_list2.append(action[1])
  action_list3.append(action[2])
  action_list4.append(action[3])
  action_list5.append(action[4])

print(action_list)

plt.title("Optimal tau for state [0,0,1,0,1]")
plt.ylim([0.35, 0.55])
plt.xlim([1, 150])


plt.plot(action_list1)
plt.plot(action_list2)
plt.plot(action_list3)
plt.plot(action_list4)
plt.plot(action_list5)
plt.xlabel("Episode")
plt.ylabel("Action")
plt.grid(True, linewidth=0.1, color='#ff0000', linestyle='-')


plt.show()

arr_20=[5476.07425020507,2055.3288128304553,1941.1555385919055,1288.7486675115529,1120.0727872788295]

arr_30=[1.5727872788295,1.6486675115529,1.781555385919055,1.9288128304553,2.3476, 2.5018739]

arr_20=arr_20[::-1]
print(len(arr_20))

import matplotlib.pyplot as plt
x= [5,10,15,20,25, 30]
print(len(x))
plt.grid(True, linewidth=0.5, color='#ff0000', linestyle='-')
plt.title("EE performance v/s D2D Pairs")
plt.plot(x,arr_30)
plt.xlabel("No. of D2D pairs")
plt.ylabel("EE performance(bits/J/Hz)")
plt.show()

import matplotlib.pyplot as plt
x= [0.2,0.4,0.6, 0.8, 1.0]
y= [2.26, 2.00, 1.92, 1.8, 1.5]
plt.grid(True, linewidth=0.5, color='#ff0000', linestyle='-')
plt.title("EE performance v/s Rmin")
plt.plot(x,y)
plt.xlabel("Rmin")
plt.ylabel("EE performance(bits/J/Hz)")
plt.show()